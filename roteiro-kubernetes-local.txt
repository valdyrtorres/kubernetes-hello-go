** Instalar o cliente kubernetes:
kubectl
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
Validando (opcional):
curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check

*** Instalando de fato:
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

kubectl version --client

Or use this for detailed view of version:

kubectl version --client --output=yaml


** Instalar o kind
go install sigs.k8s.io/kind@v0.20.0

ou

# For AMD64 / x86_64
[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
# For ARM64
[ $(uname -m) = aarch64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-arm64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind

----------

kind create cluster
kubectl cluster-info --context kind-kind

ls -l ~/.kube

kubectl get nodes

kind get clusters
kind delete clusters kind
kind get clusters
kind create cluster --config=k8s/kind.yaml --name=fullcycle

kubectl cluster-info --context kind-fullcycle

kubectl cluster-info

kubectl config get-clusters

kubectl config use-context cluster-fulano-de-tal

Obs: Descobri qual serviço usa a porta 80
sudo lsof -i -P -n | grep LISTEN
sudo netstat -tunap | grep :80
sudo fuser 80/tcp
sudo lsof -i tcp:80 -s tcp:listen

sudo /etc/init.d/apache2 stop
sudo systemctl disable apache2

go run server.go
docker build -t valdyrtorres/hello-go .
docker run --rm -p 80:80 valdyrtorres/hello-go
--------------------------------------------------
kubectl apply -f k8s/pod.yaml
kubectl get pod

kubectl port-forward pod/goserver 8000:80

kubectl delete pod goserver

kubectl apply -f k8s/replicaset.yaml

kubectl get pods

kubectl get replicasets

kubectl delete pod goserver-vhp82

kubectl describe pod goserver-fw22w

kubectl delete pod goserver-fw22w

kubectl describe pod goserver-q4w28 

ATENÇÃO: O replicaset tem um problema que quando você atualiza o yaml, ele não contempla a nova versão, 
a não ser que você delete um pod, aí outro é criado com a nova configuração. Mas, tem como resolver isso: Objeto Deployment

Deployment -> ReplicaSet -> Pod

kubectl apply -f k8s/deployment.yaml

kubectl get deployments

kubectl get replicasets

kubectl rollout history deployment goserver

Voltar a replicaset anterior (rollback)
kubectl rollout undo deployment goserver

kubectl rollout undo deployment goserver --to-revision=2

kubectl describe deployment goserver
---------------------------------------------------
Service X -> Atua como um load balance de aplicativos
kubectl apply -f k8s/service.yaml

kubectl get services ou kubectl get svc

kubectl port-forward svc/goserver-service 8000:80

Listagem das api's do kubernetes:
kubectl proxy --port=8080

kubectl get svc

kubectl delete svc goserver-service
-----------------------------------------
VARIÁVEIS DE AMBIENTE
kubectl apply -f k8s/deployment.yaml
kubectl port-forward svc/goserver-service 9000:80
------------------------------------------
Arquivo de configuração com os dados somente:
kubectl apply -f k8s/configmap-env.yaml
kubectl apply -f k8s/deployment.yaml

kubectl port-forward svc/goserver-service 9000:80
-------------------------------------------------
Dinâmico:
docker build -t valdyrtorres/hello-go:v5 .
kubectl apply -f k8s/configmap-family.yaml
kubectl apply -f k8s/deployment.yaml
kubectl get pods
kubectl port-forward svc/goserver-service 9000:80

kubectl delete deploy goserver
kubectl apply -f k8s/deployment.yaml

Acessando o POD:
kubectl exec -it goserver-75b9c9b777-447pq -- bash

kubectl logs goserver-75b9c9b777-447pq

docker build -t valdyrtorres/hello-go:v5.1 .
docker push valdyrtorres/hello-go:v5.1
kubectl apply -f k8s/deployment.yaml
kubectl port-forward svc/goserver-service 9000:80
----------------------------------
Secret:
docker build -t valdyrtorres/hello-go:v5.3 .
docker push valdyrtorres/hello-go:v5.3
Todos os secrets dentro de secret.yaml devem estar em base64
O comando abaixo em bash linux te dá o correspondente em base64:
echo "valdir" | base64
echo "1234567" | base64

Criando o secret:
kubectl apply -f k8s/secret.yaml

kubectl apply -f k8s/deployment.yaml
kubectl port-forward svc/goserver-service 9000:80

Verificando:
kubectl get pods
kubectl exec -it goserver-65684d9556-n2g45 -- bash
echo $USER
Retorna valdir
-------------------------------------------------
HEALTH
docker build -t valdyrtorres/hello-go:v5.4 .
docker push valdyrtorres/hello-go:v5.4
kubectl delete deploy goserver
kubectl apply -f k8s/deployment.yaml
kubectl port-forward svc/goserver-service 9000:80
-------------------------------------------------
livenessProbe
kubectl delete deploy goserver
NO LINUX:
kubectl apply -f k8s/deployment.yaml && watch -h1 kubectl get pods
equivalente no Powershell:
kubectl apply -f k8s/deployment.yaml ; while (1) {kubectl get pods; sleep 2; cls}
-------------------------------------------------
readiness
kubectl describe pod goserver-5fc767876c-v2hpx
docker build -t valdyrtorres/hello-go:v5.5 .
kubectl delete deploy goserver
kubectl delete deploy goserver; kubectl apply -f k8s/deployment.yaml
kubectl port-forward svc/goserver-service 9000:80

kubectl delete deploy goserver
kubectl apply -f k8s/deployment.yaml ; while (1) {kubectl get pods; sleep 2; cls}
---------------------------------
Combinando Liveness e Readdiness
kubectl apply -f k8s/deployment.yaml ; while (1) {kubectl get pods; sleep 2; cls}
docker build -t valdyrtorres/hello-go:v5.6 .
docker push valdyrtorres/hello-go:v5.6
---------------------------------
Trabalhando com startupProbe
kubectl apply -f k8s/deployment.yaml ; while (1) {kubectl get pods; sleep 2; cls}
---------------------------------
Aparentemente o metrics-server não funciona bem por trás um proxy
HPA - Horizontal Pointing Autoscaling
kubectl apply -f k8s/metrics-server.yaml
kubectl get apiservices
kubectl delete -f k8s/metrics-server.yaml 

kubectl describe deployment metrics-server -n kube-system

kubectl get pods -n kube-system

kubectl logs metrics-server-75f45b4dd4-96p8b -n kube-system

kubectl patch deployment metrics-server -n kube-system --type 'json' -p '[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'
-----------------------------------
Deployment com resources
kubectl apply -f k8s/deployment.yaml 
kubectl get pods
kubectl top pod goserver-6b5d476468-bpfsv
-------------------------------------
HPA
kubectl apply -f k8s/hpa.yaml
kubectl get hpa
--------------------
Teste de stress com fortio (https://github.com/fortio/fortio)
docker build -t valdyrtorres/hello-go:v9.6 .
Com a atualização do kubectl para a versão 1.21, o parâmetro --generator do comando

$ kubectl run -it --generator=run-pod/v1 fortio --rm --image=fortio/fortio -- load -qps 800 -t 120s -c 70 "http://goserver-service/healthz"
passou a não ser mais suportado, apresentando o erro "Error: unknown flag: --generator".

Para a realização do teste, execute o comando sem este parâmetro, ficando da seguinte maneira:

$ kubectl run -it fortio --rm --image=fortio/fortio -- load -qps 800 -t 120s -c 70 "http://goserver-service/healthz"
Com isto, será possível ver os pods escalando no teste de stress.

while (1) {kubectl get hpa; sleep 2; cls}
watch -n1 kubectl get hpa

kubectl apply -f k8s/deployment.yaml
----------------------------------------
Entendendo volumes persistentes
- StorageClass
#DStorageClass -> AWS
Claim -> StorageClass -> Disponibilizar o espaco que eu preciso - BlockStorage

kubectl get storageclass
kubectl config get-contexts

kubectl apply -f k8s/pvc.yaml
kubectl get pvc

kubectl apply -f k8s/deployment.yaml
kubectl get pvc

kubectl get po
kubectl exec -it goserver-5cbbd7db8d-dlgfh -- bash
kubectl delete pod goserver-5cbbd7db8d-dlgfh
kubectl get po
kubectl exec -it goserver-5cbbd7db8d-9whm5 -- bash
-------------------------------------
CRIANDO STATEFULSET
kubectl apply -f k8s/statefulset.yaml
kubectl delete -f k8s/statefulset.yaml
kubectl get pods
kubectl logs mysql-f99c5456f-7snhz

kubectl apply -f k8s/statefulset.yaml
kubectl get pods

kubectl delete deploy mysql 

kubectl apply -f k8s/statefulset.yaml
kubectl get pods

kubectl delete -f k8s/statefulset.yaml
kubectl apply -f k8s/statefulset.yaml

kubectl scale statefulset mysql --replicas=5
-----------------------
Aula - Criando headless service
kubectl delete statefulset mysql
kubectl apply -f k8s/mysql-service-h.yaml

kubectl get pods
kubectl get svc

kubectl delete service mysql-h
kubectl apply -f k8s/mysql-service-h.yaml
kubectl get svc

kubectl exec -it goserver-5cbbd7db8d-pr7cp -- bash
ping mysql-h

O headless server permite que eu consiga chamar exatamente o pod de mysql que eu quero:
ping mysql-h
ping mysql-0.mysql-h
ping mysql-3.mysql-h
Ex de saída: 
64 bytes from mysql-3.mysql-h.default.svc.cluster.local (10.244.2.4): icmp_seq=3 ttl=62 time=0.115 ms
------------------------
Volumes dinamicos com statefulset
kubectl delete statefulset mysql
kubectl apply -f k8s/statefulset.yaml
kubectl get pods
Testando o volumes
kubectl get pvc
Teste:
kubectl delete pod mysql-1
kubectl get pvc
--------------------------
Vale a pena salvar o meu banco de dados na estrutura do kubernetes?
Opinião do Wesley: Para aplicações críticas, hoje é mais seguro deixar em um RDS por exemplo fora do kubernetes em um lugar gerenciado
Isso é mais por criticidade de banco de dados e não pelo kubernetes
--------------------------
Ingres - serviço do kubernetes que é o ponto único de entrada da tua aplicação e faz o roteamento do nosso serviço
---------------------------
Criar uma conta no google para tentar trabalhar sem custos:
valdyr.gcp@gmail.com
Projeto Fullcycle
Trial começando em 07/09/2023 com $300 nos 3 primeiros meses
Instalar o gcloud CLI e executar o comando:
gcloud auth login
gcloud config set project projeto-fullcycle-398320
gcloud components install gke-gcloud-auth-plugin
gcloud container clusters get-credentials curso-fullcycle --zone us-central1-c --project projeto-fullcycle-398320

kubectl apply -f k8s/
kubectl get pod
kubectl get svc
---------------------------
Instalar Ingres Nginx controller:
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm install ingress-nginx ingress-nginx/ingress-nginx

Verificar instalação:
kubectl get svc
kubectl get pod
---------------------------------
Configurando Ingress e DNS:
kubectl apply -f k8s/ingress.yaml

kubectl get crd | grep ingress
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml

kubectl delete pods -n ingress-nginx --selector=app.kubernetes.io/component=controller
kubectl delete -f k8s/ingress-nginx-configmap-reader.yaml
kubectl delete -f k8s/service-account-nginx.yaml

kubectl delete svc goserver-service
kubectl apply -f k8s/service.yaml

kubectl exec -it goserver-5cbbd7db8d-dlgfh -- bash

kubectl delete -f k8s/secret.yaml
-------------------------------------------
CERT-MANAGER
Ler docs

https://cert-manager.io/docs/installation/kubernetes
-------------------------
Namespaces e Service Accounts
kubectl get ns
kubectl get pods -n=kube-system

Criar namespace no kubernetes:
kubectl create ns dev

Provisionar deployment.yaml (d.yaml) no namespace dev:
OBS: O namespace pode ser informado dentro do deployment.yaml na seção spec
kubectl apply -f d.yaml -n=dev
-----------------------
cd namespaces
kubectl apply -f deployment.yaml
kubectl apply -f deployment.yaml -n=dev

kubectl create namespace prod

kubectl apply -f deployment.yaml -n=prod

kubectl get pods -n=prod
kubectl get pods -n=dev

kubectl get pods -l app=server

VER AS CONFIGURAÇÕES NO KUBERNETES:
kubectl config view

kubectl config current-context

kubectl config set-context 

kubectl config set-context dev --namespace=dev --cluster=kind-fullcycle --user=kind-fullcycle

kubectl config set-context prod --namespace=prod --cluster=kind-fullcycle --user=kind-fullcycle

kubectl config view

kubectl config use-context dev

kubectl config current-context

kubectl get pods

kubectl delete deployment server -n=default

kubectl delete deployment server -n=prod

kubectl get pods
kubectl config use-context prod
---------------------
kubectl get serviceaccounts
kubectl config use-context kind-fullcycle
kubectl get serviceaccounts

kubectl apply -f security.yaml

kubectl get serviceaccounts

kubectl api-resources
o comando acima vai listar:
...
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAcce
...

kubectl apply -f security.yaml

(base) valdir@valdir-debian:~/devcode/fullcycle/kubernetes/k8s/namespaces$ kubectl apply -f security.yaml
serviceaccount/server unchanged
role.rbac.authorization.k8s.io/server-read unchanged
rolebinding.rbac.authorization.k8s.io/server-read-bind created

kubectl apply -f deployment.yaml

kubectl get pods

kubectl describe pod server-6676fff8d4-nzc5q

Somente Role é em nível de namespace
ClusterRole é em nível de cluster


